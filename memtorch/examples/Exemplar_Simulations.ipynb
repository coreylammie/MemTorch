{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplar Simulations\n",
    "Exemplar simulations investigating the performance degradation of Memristive Deep Neural Networks (MDNNs) when non-ideal device characteristics are accounted for using the CIFAR-10 dataset are provided below. Results can be plotted using `plot_all_exemplar.m`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the MobileNetV2 Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, expansion, stride):\n",
    "        super(Block, self).__init__()\n",
    "        self.stride = stride\n",
    "        planes = expansion * in_planes\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn1 = nn.BatchNorm2d(planes, affine=False)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes, affine=False)\n",
    "        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes, affine=False)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 1 and in_planes != out_planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0),\n",
    "                nn.BatchNorm2d(out_planes, affine=False),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out = out + self.shortcut(x) if self.stride==1 else out\n",
    "        return out\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    cfg = [(1,  16, 1, 1),\n",
    "           (6,  24, 2, 1),\n",
    "           (6,  32, 3, 2),\n",
    "           (6,  64, 4, 2),\n",
    "           (6,  96, 3, 1),\n",
    "           (6, 160, 3, 2),\n",
    "           (6, 320, 1, 1)]\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32, affine=False)\n",
    "        self.layers = self._make_layers(in_planes=32)\n",
    "        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0)\n",
    "        self.bn2 = nn.BatchNorm2d(1280, affine=False)\n",
    "        self.linear = nn.Linear(1280, num_classes)\n",
    "\n",
    "    def _make_layers(self, in_planes):\n",
    "        layers = []\n",
    "        for expansion, out_planes, num_blocks, stride in self.cfg:\n",
    "            strides = [stride] + [1]*(num_blocks-1)\n",
    "            for stride in strides:\n",
    "                layers.append(Block(in_planes, out_planes, expansion, stride))\n",
    "                in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layers(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train MobileNetV2 Using CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from mobilenetv2 import MobileNetV2\n",
    "\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "set_all_seeds(0)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "epochs = 100\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=256, shuffle=True, num_workers=1)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=128, shuffle=False, num_workers=1)\n",
    "model = MobileNetV2().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.1)\n",
    "best_accuracy = 0\n",
    "for epoch in range(0, epochs):\n",
    "    print('Epoch: [%d]\\t\\t' % (epoch + 1), end='')\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    model.eval()\n",
    "    accuracy = test(model, test_loader)\n",
    "    print('%2.2f%%' % accuracy)\n",
    "    if accuracy > best_accuracy:\n",
    "        print('Saving model...')\n",
    "        torch.save(model.state_dict(), 'trained_model.pt')\n",
    "        best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Figure 1 [A,E]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import memtorch\n",
    "from memtorch.utils import LoadCIFAR10\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mobilenetv2 import MobileNetV2\n",
    "import torchvision\n",
    "import copy\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, num_workers=1)\n",
    "model = MobileNetV2().to(device)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "    model.eval()\n",
    "except:\n",
    "    raise Exception('trained_model.pt has not been found.')\n",
    "\n",
    "print('Test Set Accuracy: \\t%2.2f%%' % test(model, test_loader))\n",
    "\n",
    "model = MobileNetV2().to(device)\n",
    "model.load_state_dict(torch.load('trained_model.pt'), strict=True)\n",
    "model.eval()\n",
    "print(test(model, test_loader))\n",
    "r_on = 1.4e4\n",
    "r_off = 5e7\n",
    "\n",
    "def trial(r_on, r_off, tile_shape, ADC_resolution, sigma):\n",
    "    model_ = copy.deepcopy(model)\n",
    "    reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "    if sigma == 0.:\n",
    "        reference_memristor_params = {'time_series_resolution': 1e-10, 'r_off': r_off, 'r_on': r_on}\n",
    "    else:\n",
    "        reference_memristor_params = {'time_series_resolution': 1e-10,\n",
    "                                  'r_off': memtorch.bh.StochasticParameter(loc=r_off, scale=sigma*2, min=1),\n",
    "                                  'r_on': memtorch.bh.StochasticParameter(loc=r_on, scale=sigma, min=1)}\n",
    "\n",
    "    patched_model = patch_model(copy.deepcopy(model_),\n",
    "                              memristor_model=reference_memristor,\n",
    "                              memristor_model_params=reference_memristor_params,\n",
    "                              module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                              mapping_routine=naive_map,\n",
    "                              transistor=True,\n",
    "                              programming_routine=None,\n",
    "                              scheme=memtorch.bh.Scheme.DoubleColumn,\n",
    "                              tile_shape=tile_shape,\n",
    "                              max_input_voltage=0.3,\n",
    "                              ADC_resolution=int(ADC_resolution),\n",
    "                              ADC_overflow_rate=0.,\n",
    "                              quant_method='linear')\n",
    "    \n",
    "    patched_model.tune_()\n",
    "    return test(patched_model, test_loader)\n",
    "\n",
    "df = pd.DataFrame(columns=['tile_shape', 'ADC_resolution', 'sigma', 'test_set_accuracy'])\n",
    "tile_shape = [(256, 64)]\n",
    "ADC_resolution = np.linspace(2, 10, num=5, endpoint=True, dtype=int)\n",
    "sigma = np.logspace(6, 7, endpoint=True, num=5)\n",
    "for tile_shape_ in tile_shape:\n",
    "    for ADC_resolution_ in ADC_resolution:\n",
    "        for sigma_ in sigma:\n",
    "            print('tile_shape: %s; ADC_resolution: %d; sigma: %d' % (tile_shape_, ADC_resolution_, sigma_))\n",
    "            df = df.append({'tile_shape': tile_shape_, \n",
    "                            'ADC_resolution': ADC_resolution_, \n",
    "                            'sigma': sigma_, \n",
    "                            'test_set_accuracy': trial(r_on, r_off, tile_shape_, ADC_resolution_, sigma_)}, ignore_index=True)\n",
    "            df.to_csv('1_AE.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Figure 1 [B,F]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import memtorch\n",
    "from memtorch.utils import LoadCIFAR10\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mobilenetv2 import MobileNetV2\n",
    "import torchvision\n",
    "import copy\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, num_workers=1)\n",
    "model = MobileNetV2().to(device)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "    model.eval()\n",
    "except:\n",
    "    raise Exception('trained_model.pt has not been found.')\n",
    "\n",
    "print('Test Set Accuracy: \\t%2.2f%%' % test(model, test_loader))\n",
    "\n",
    "model = MobileNetV2().to(device)\n",
    "model.load_state_dict(torch.load('trained_model.pt'), strict=True)\n",
    "model.eval()\n",
    "print(test(model, test_loader))\n",
    "r_on = 1.4e4\n",
    "r_off = 5e7\n",
    "\n",
    "def trial(r_on, r_off, tile_shape, ADC_resolution, conductance_states):\n",
    "    model_ = copy.deepcopy(model)\n",
    "    reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "    reference_memristor_params = {'time_series_resolution': 1e-10, 'r_off': r_off, 'r_on': r_on}\n",
    "    patched_model = patch_model(copy.deepcopy(model_),\n",
    "                              memristor_model=reference_memristor,\n",
    "                              memristor_model_params=reference_memristor_params,\n",
    "                              module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                              mapping_routine=naive_map,\n",
    "                              transistor=True,\n",
    "                              programming_routine=None,\n",
    "                              scheme=memtorch.bh.Scheme.DoubleColumn,\n",
    "                              tile_shape=tile_shape,\n",
    "                              max_input_voltage=0.3,\n",
    "                              ADC_resolution=int(ADC_resolution),\n",
    "                              ADC_overflow_rate=0.,\n",
    "                              quant_method='linear')\n",
    "\n",
    "    patched_model = apply_nonidealities(patched_model,\n",
    "                                        non_idealities=[memtorch.bh.nonideality.NonIdeality.FiniteConductanceStates],\n",
    "                                        conductance_states = int(conductance_states))\n",
    "    \n",
    "    patched_model.tune_()\n",
    "    return test(patched_model, test_loader)\n",
    "\n",
    "df = pd.DataFrame(columns=['tile_shape', 'ADC_resolution', 'conductance_states', 'test_set_accuracy'])\n",
    "torch.backends.cudnn.benchmark = False\n",
    "tile_shape = [(128, 128), (256, 64)]\n",
    "ADC_resolution = np.linspace(2, 10, num=5, endpoint=True, dtype=int)\n",
    "conductance_states = np.linspace(2, 10, num=5, endpoint=True, dtype=int)\n",
    "for tile_shape_ in tile_shape:\n",
    "    for ADC_resolution_ in ADC_resolution:\n",
    "        for conductance_states_ in conductance_states:\n",
    "            print('tile_shape: %s; ADC_resolution: %d; conductance_states: %d' % (tile_shape_, ADC_resolution_, conductance_states_))\n",
    "            df = df.append({'tile_shape': tile_shape_, \n",
    "                            'ADC_resolution': ADC_resolution_, \n",
    "                            'conductance_states': conductance_states_, \n",
    "                            'test_set_accuracy': trial(r_on, r_off, tile_shape_, ADC_resolution_, conductance_states_)}, ignore_index=True)\n",
    "            df.to_csv('1_BF.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Figure 1 [C,G]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import memtorch\n",
    "from memtorch.utils import LoadCIFAR10\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mobilenetv2 import MobileNetV2\n",
    "import torchvision\n",
    "import copy\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, num_workers=1)\n",
    "model = MobileNetV2().to(device)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "    model.eval()\n",
    "except:\n",
    "    raise Exception('trained_model.pt has not been found.')\n",
    "\n",
    "print('Test Set Accuracy: \\t%2.2f%%' % test(model, test_loader))\n",
    "\n",
    "model = MobileNetV2().to(device)\n",
    "model.load_state_dict(torch.load('trained_model.pt'), strict=True)\n",
    "model.eval()\n",
    "print(test(model, test_loader))\n",
    "r_on = 1.4e4\n",
    "r_off = 5e7\n",
    "\n",
    "def trial(r_on, r_off, tile_shape, ADC_resolution, failure_percentage):\n",
    "    model_ = copy.deepcopy(model)\n",
    "    reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "    reference_memristor_params = {'time_series_resolution': 1e-10, 'r_off': r_off, 'r_on': r_on}\n",
    "    patched_model = patch_model(copy.deepcopy(model_),\n",
    "                              memristor_model=reference_memristor,\n",
    "                              memristor_model_params=reference_memristor_params,\n",
    "                              module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                              mapping_routine=naive_map,\n",
    "                              transistor=True,\n",
    "                              programming_routine=None,\n",
    "                              scheme=memtorch.bh.Scheme.DoubleColumn,\n",
    "                              tile_shape=tile_shape,\n",
    "                              max_input_voltage=0.3,\n",
    "                              ADC_resolution=int(ADC_resolution),\n",
    "                              ADC_overflow_rate=0.,\n",
    "                              quant_method='linear')\n",
    "\n",
    "    patched_model = apply_nonidealities(patched_model,\n",
    "                                        non_idealities=[memtorch.bh.nonideality.NonIdeality.DeviceFaults],\n",
    "                                        lrs_proportion=failure_percentage,\n",
    "                                        hrs_proportion=0.,\n",
    "                                        electroform_proportion=0.)\n",
    "    \n",
    "    patched_model.tune_()\n",
    "    return test(patched_model, test_loader)\n",
    "\n",
    "df = pd.DataFrame(columns=['tile_shape', 'ADC_resolution', 'failure_percentage', 'test_set_accuracy'])\n",
    "tile_shape = [(128, 128), (256, 64)]\n",
    "ADC_resolution = np.linspace(2, 10, num=5, endpoint=True, dtype=int)\n",
    "failure_percentage = np.linspace(0, 0.25, 5)\n",
    "for tile_shape_ in tile_shape:\n",
    "    for ADC_resolution_ in ADC_resolution:\n",
    "        for failure_percentage_ in failure_percentage:\n",
    "            print('tile_shape: %s; ADC_resolution: %d; failure_percentage: %d' % (tile_shape_, ADC_resolution_, failure_percentage_))\n",
    "            df = df.append({'tile_shape': tile_shape_, \n",
    "                            'ADC_resolution': ADC_resolution_, \n",
    "                            'failure_percentage': failure_percentage_, \n",
    "                            'test_set_accuracy': trial(r_on, r_off, tile_shape_, ADC_resolution_, failure_percentage_)}, ignore_index=True)\n",
    "            df.to_csv('1_CG.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Figure 1 [D,H]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import memtorch\n",
    "from memtorch.utils import LoadCIFAR10\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from mobilenetv2 import MobileNetV2\n",
    "import torchvision\n",
    "import copy\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=256, shuffle=False, num_workers=1)\n",
    "model = MobileNetV2().to(device)\n",
    "try:\n",
    "    model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "    model.eval()\n",
    "except:\n",
    "    raise Exception('trained_model.pt has not been found.')\n",
    "\n",
    "print('Test Set Accuracy: \\t%2.2f%%' % test(model, test_loader))\n",
    "\n",
    "model = MobileNetV2().to(device)\n",
    "model.load_state_dict(torch.load('trained_model.pt'), strict=True)\n",
    "model.eval()\n",
    "print(test(model, test_loader))\n",
    "r_on = 1.4e4\n",
    "r_off = 5e7\n",
    "\n",
    "def trial(r_on, r_off, tile_shape, ADC_resolution, failure_percentage):\n",
    "    model_ = copy.deepcopy(model)\n",
    "    reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "    reference_memristor_params = {'time_series_resolution': 1e-10, 'r_off': r_off, 'r_on': r_on}\n",
    "    patched_model = patch_model(copy.deepcopy(model_),\n",
    "                              memristor_model=reference_memristor,\n",
    "                              memristor_model_params=reference_memristor_params,\n",
    "                              module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                              mapping_routine=naive_map,\n",
    "                              transistor=True,\n",
    "                              programming_routine=None,\n",
    "                              scheme=memtorch.bh.Scheme.DoubleColumn,\n",
    "                              tile_shape=tile_shape,\n",
    "                              max_input_voltage=0.3,\n",
    "                              ADC_resolution=int(ADC_resolution),\n",
    "                              ADC_overflow_rate=0.,\n",
    "                              quant_method='linear')\n",
    "\n",
    "    patched_model = apply_nonidealities(patched_model,\n",
    "                                        non_idealities=[memtorch.bh.nonideality.NonIdeality.DeviceFaults],\n",
    "                                        lrs_proportion=0.,\n",
    "                                        hrs_proportion=failure_percentage,\n",
    "                                        electroform_proportion=0.)\n",
    "    \n",
    "    patched_model.tune_()\n",
    "    return test(patched_model, test_loader)\n",
    "\n",
    "df = pd.DataFrame(columns=['tile_shape', 'ADC_resolution', 'failure_percentage', 'test_set_accuracy'])\n",
    "tile_shape = [(128, 128), (256, 64)]\n",
    "ADC_resolution = np.linspace(2, 10, num=5, endpoint=True, dtype=int)\n",
    "failure_percentage = np.linspace(0, 0.25, 5)\n",
    "for tile_shape_ in tile_shape:\n",
    "    for ADC_resolution_ in ADC_resolution:\n",
    "        for failure_percentage_ in failure_percentage:\n",
    "            print('tile_shape: %s; ADC_resolution: %d; failure_percentage: %d' % (tile_shape_, ADC_resolution_, failure_percentage_))\n",
    "            df = df.append({'tile_shape': tile_shape_, \n",
    "                            'ADC_resolution': ADC_resolution_, \n",
    "                            'failure_percentage': failure_percentage_, \n",
    "                            'test_set_accuracy': trial(r_on, r_off, tile_shape_, ADC_resolution_, failure_percentage_)}, ignore_index=True)\n",
    "            df.to_csv('1_DH.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
