{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qBYEsnSav5E1"
   },
   "source": [
    "## 1. Training and benchmarking a DNN using CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCy9QqPHv5E3"
   },
   "source": [
    "The VGG-16 DNN architecture is trained and tested using the CIFAR-10 data set. The CIFAR-10 data set consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. The network is trained for 50 epochs with a batch size, $\\Im=256$. The initial learning rate is $\\eta = 1e-2$, which is decayed by an order of magnitude every 20 training epochs. Adam is used to optimize network parameters and Cross Entropy (CE) is used to determine network losses. *memtorch.utils.LoadCIFAR10* is used to load the CIFAR-10 training and test sets. After each epoch the model is bench-marked using the CIFAR-10 test set. The model that achieves the highest test set accuracy is saved as *trained_model.pt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVH_tu3tv5E4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import memtorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from memtorch.utils import LoadCIFAR10\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, inflation_ratio=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=128*inflation_ratio, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn0 = nn.BatchNorm2d(num_features=128*inflation_ratio)\n",
    "        self.act0 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=128*inflation_ratio, out_channels=128*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=128*inflation_ratio)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=128*inflation_ratio, out_channels=256*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256*inflation_ratio)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(in_channels=256*inflation_ratio, out_channels=256*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256*inflation_ratio)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(in_channels=256*inflation_ratio, out_channels=512*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=512*inflation_ratio)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.conv5 = nn.Conv2d(in_channels=512*inflation_ratio, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=512)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(in_features=512*4*4, out_features=1024)\n",
    "        self.bn6 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.act6 = nn.ReLU()\n",
    "        self.fc7 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.act7 = nn.ReLU()\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.act0(self.bn0(self.conv0(input)))\n",
    "        x = self.act1(self.bn1(F.max_pool2d(self.conv1(x), 2)))\n",
    "        x = self.act2(self.bn2(self.conv2(x)))\n",
    "        x = self.act3(self.bn3(F.max_pool2d(self.conv3(x), 2)))\n",
    "        x = self.act4(self.bn4(self.conv4(x)))\n",
    "        x = self.act5(self.bn5(F.max_pool2d(self.conv5(x), 2)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act6(self.bn6(self.fc6(x)))\n",
    "        x = self.act7(self.bn7(self.fc7(x)))\n",
    "        return self.fc8(x)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):        \n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "epochs = 50\n",
    "train_loader, validation_loader, test_loader = LoadCIFAR10(batch_size=256, validation=False)\n",
    "model = Net().to(device)\n",
    "if device == 'cuda':\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "best_accuracy = 0\n",
    "for epoch in range(0, epochs):\n",
    "    print('Epoch: [%d]\\t\\t' % (epoch + 1), end='')\n",
    "    if epoch % 20 == 0:\n",
    "        learning_rate = learning_rate * 0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy = test(model, test_loader)\n",
    "    print('%2.2f%%' % accuracy)\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'trained_model.pt')\n",
    "        best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ag8Z6Rn_v5E8"
   },
   "source": [
    "## 2. Conversion of a DNN to a MDNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyt9qHvAv5E9"
   },
   "source": [
    "We use MemTorch to demonstrate the conversion of a DNN to MDNN. A memristive device model is defined and characterized below, which is used to replace all *torch.nn.Linear* layers within the DNN, trained in Step 1, with equivalent crossbar architectures using *memtorch.mn.Module.patch_model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "dRMGKP-lv5E-",
    "outputId": "f22a800e-3eae-4060-a874-23d7a218f3cf"
   },
   "outputs": [],
   "source": [
    "reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "reference_memristor_params = {'time_series_resolution': 1e-10}\n",
    "memristor = reference_memristor(**reference_memristor_params)\n",
    "memristor.plot_hysteresis_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0kLErl0v5FC"
   },
   "source": [
    "*memtorch.bh.map.Parameter.naive_map* is used to convert the weights within all *torch.nn.Linear* layers to equivalent conductance values, to be programmed to the two memristive devices used to represent each weight (positive and negative) using Eq. (13). \n",
    "\n",
    "*transistor* is *True*, so a 1T1R arrangement is simulated. *programming_routine* is set to *None* to skip device-level simulation of the programming routine. We note if *transistor* is *False* *programming_routine* must not be *None*. In which case, device-level simulation is performed for each device using *memtorch.bh.crossbar.gen_programming_signal* and *memtorch.bh.memristor.Memristor.simulate*, which uses finite differences to model internal device dynamics. As *scheme* is not defined, a double-column parameter representation scheme is adopted.\n",
    "\n",
    "All patched *torch.nn.Linear* layers are tuned using linear regression. A randomly generated tensor of size (8, *self.in_features*) is propagated through each memristive layer and each legacy layer (accessible using *layer.forward_legacy*). *sklearn.linear_model.LinearRegression* is used to determine the coefficient and intercept between the linear relationship of each set of outputs, which is used to define the *transform_output* lamdba function, that maps the output of each layer to their equivalent representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJWSTW5Qv5FD"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "\n",
    "\n",
    "model = Net()\n",
    "if torch.cuda.is_available():\n",
    "  model = torch.nn.DataParallel(model)\n",
    "\n",
    "model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "patched_model = patch_model(copy.deepcopy(model),\n",
    "                          memristor_model=reference_memristor,\n",
    "                          memristor_model_params=reference_memristor_params,\n",
    "                          module_parameters_to_patch=[torch.nn.Linear],\n",
    "                          mapping_routine=naive_map,\n",
    "                          transistor=True,\n",
    "                          programming_routine=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mam3ggffv5FG"
   },
   "outputs": [],
   "source": [
    "patched_model.tune_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5F0muXPv5FK"
   },
   "outputs": [],
   "source": [
    "print(test(patched_model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eV8IJSH6v5FN"
   },
   "source": [
    "## 3. Modeling non-ideal device characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4H9f4d248V7"
   },
   "source": [
    "We use a simple prototype model to demonstrate modeling non-ideal device characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "x0mghWrdv5FO",
    "outputId": "fb7d0542-7839-438e-94fb-029d05f0bbc1"
   },
   "outputs": [],
   "source": [
    "from memtorch.mn.Module import patch_model\n",
    "import copy\n",
    "from memtorch.map.Parameter import naive_map\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.convolutional_layer = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=5)\n",
    "        self.linear_layer = torch.nn.Linear(in_features=16, out_features=4)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.convolutional_layer(input)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.linear_layer(x)\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "model = Model()\n",
    "if torch.cuda.is_available():\n",
    "  model = torch.nn.DataParallel(model)\n",
    "\n",
    "reference_memristor_params = {'time_series_resolution': 1e-10, 'r_off': 200, 'r_on': 100}\n",
    "patched_model = patch_model(copy.deepcopy(model),\n",
    "                          memristor_model=reference_memristor,\n",
    "                          memristor_model_params=reference_memristor_params,\n",
    "                          module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                          mapping_routine=naive_map,\n",
    "                          transistor=True,\n",
    "                          programming_routine=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2Vcpuuw5D_S"
   },
   "source": [
    "Device-device variability is introduced to the VTEAM reference memristor model using *memtorch.bh.StochasticParameter*, by sampling $R_{\\text{OFF}}$ for each device from a normal distribution with $\\sigma = 20$ and $x = 200$, and  $R_{\\text{ON}}$ for each device from a normal distribution with $\\sigma = 10$ and $x = 100$. Using *np.vectorize*, the $R_{\\text{OFF}}$ and $R_{\\text{ON}}$ values for each memristive device are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "4thsaVDEv5FS",
    "outputId": "e46021ed-b57f-4be7-f29e-15bd17ec2473"
   },
   "outputs": [],
   "source": [
    "from memtorch.mn.Module import patch_model\n",
    "import copy\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "reference_memristor_params = {'time_series_resolution': 1e-10, \n",
    "                              'r_off': memtorch.bh.StochasticParameter(200, std=20, min=2),\n",
    "                              'r_on': memtorch.bh.StochasticParameter(100, std=10, min=1)}\n",
    "\n",
    "patched_model_ = patch_model(copy.deepcopy(model),\n",
    "                          memristor_model=reference_memristor,\n",
    "                          memristor_model_params=reference_memristor_params,\n",
    "                          module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                          mapping_routine=naive_map,\n",
    "                          transistor=True,\n",
    "                          programming_routine=None)\n",
    "\n",
    "A = torch.Tensor(np.vectorize(lambda x: x.r_off)(patched_model_.linear_layer.crossbars[0].devices))\n",
    "B = torch.Tensor(np.vectorize(lambda x: x.r_on)(patched_model_.linear_layer.crossbars[0].devices))\n",
    "C = torch.cat((A, B), 0)\n",
    "\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(A.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(B.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "plt.savefig('var.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3okdYrSZ7dJh"
   },
   "source": [
    "We model a number (5) of finite discrete conductance states using $memtorch.bh.nonideality.NonIdeality.FiniteConductanceStates$. The conductance levels within the positive crossbar were compared before and after a finite discrete conductance states are modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "id": "bpIOctS5v5FV",
    "outputId": "feefbf64-f8fe-4da2-8212-afa17dade373"
   },
   "outputs": [],
   "source": [
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "\n",
    "A = 1 / patched_model.linear_layer.crossbars[0].conductance_matrix\n",
    "model = apply_nonidealities(copy.deepcopy(patched_model),\n",
    "                                  non_idealities=[memtorch.bh.nonideality.NonIdeality.FiniteConductanceStates],\n",
    "                                  conductance_states = 5)\n",
    "B = 1 / model.linear_layer.crossbars[0].conductance_matrix\n",
    "C = torch.cat((A, B), 0)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(A.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(B.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "plt.savefig('finite.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfyXHayL70HP"
   },
   "source": [
    "We model device failure using $memtorch.bh.nonideality.NonIdeality.DeviceFaults$. The conductance levels within the positive crossbar are once again compared, before and after  50\\% of devices are stuck at $R_{\\text{LRS}}$ and 50\\% of devices are stuck at $R_{\\text{HRS}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "id": "E6Xi7FUxv5FZ",
    "outputId": "a074acc6-3e0b-4d39-89cf-90fb2ad05bdd"
   },
   "outputs": [],
   "source": [
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "\n",
    "A = 1 / patched_model.linear_layer.crossbars[0].conductance_matrix\n",
    "model = apply_nonidealities(copy.deepcopy(patched_model),\n",
    "                                  non_idealities=[memtorch.bh.nonideality.NonIdeality.DeviceFaults],\n",
    "                                  lrs_proportion=0.5,\n",
    "                                  hrs_proportion=0.5,\n",
    "                                  electroform_proportion=0)\n",
    "B = 1 / model.linear_layer.crossbars[0].conductance_matrix\n",
    "C = torch.cat((A, B), 0)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(A.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(B.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "plt.savefig('fault.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWX0f_Wz8U1L"
   },
   "source": [
    "We model non-linear I/V characteristics using $memtorch.bh.nonideality.NonIdeality.NonLinear$ during inference. The output of the single *torch.nn.Linear* layer are compared when devices were simulated during inference with linear and non-linear I/V characteristics. Non-linear I/V characteristics were determined by applying a half-voltage sweep, using a sinusoidal cosine voltage signal with a duration of 5ns, amplitude of 1V, and a frequency of 50 MHz to each device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xIbVHttJv5Fb",
    "outputId": "559d9c25-25f0-4f6c-fbeb-75defceb9048"
   },
   "outputs": [],
   "source": [
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A = torch.tensor(np.zeros((100, 4)))\n",
    "B = torch.tensor(np.zeros((100, 4)))\n",
    "for i in range(100):\n",
    "  input = torch.zeros((1,3,8,8)).uniform_(-1, 1)\n",
    "  A[i, :] = patched_model(input)\n",
    "  model = apply_nonidealities(copy.deepcopy(patched_model),\n",
    "                              non_idealities=[memtorch.bh.nonideality.NonIdeality.NonLinear],\n",
    "                              sweep_duration=5e-9,\n",
    "                              sweep_voltage_signal_amplitude=1,\n",
    "                              sweep_voltage_signal_frequency=50e6)\n",
    "  B[i, :] = model(input)\n",
    "\n",
    "print(cosine_similarity([A.view(-1).numpy()], [B.view(-1).numpy()]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
