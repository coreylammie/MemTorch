{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemTorch Tutorial\n",
    "## Introduction\n",
    "In this tutorial, you will learn how to use MemTorch to convert Deep Neural Networks (DNNs) to Memristive Deep Neural Networks (MDNNs), and how to simulate non-ideal device characteristics and key peripheral circuitry. MemTorch is a Simulation Framework for Memristive Deep Learning Systems which integrates directly with the well-known PyTorch Machine Learning (ML) library, which is presented in *MemTorch: An Open-source Simulation Framework for Memristive Deep Learning Systems*, which has been released [here](https://arxiv.org/abs/2004.10971).\n",
    "\n",
    "<img src=\"https://github.com/coreylammie/MemTorch/blob/master/overview.svg\" width=\"100%\">\n"
   ]
  },
  {
   "source": [
    "## 1. Installation\n",
    "To install MemTorch from source:\n",
    "\n",
    "```\n",
    "git clone --recursive https://github.com/coreylammie/MemTorch\n",
    "cd MemTorch\n",
    "python setup.py install\n",
    "```\n",
    "\n",
    "*If CUDA is True in setup.py, CUDA Toolkit 10.1 and Microsoft Visual C++ Build Tools are required. If CUDA is False in setup.py, Microsoft Visual C++ Build Tools are required.*\n",
    "\n",
    "Alternatively, MemTorch can be installed using the *pip* package-management system:\n",
    "\n",
    "```\n",
    "pip install memtorch-cpu # Supports normal operation\n",
    "pip install memtorch # Supports CUDA and normal operation\n",
    "```\n",
    "\n",
    "A complete API is avaliable [here](https://memtorch.readthedocs.io/)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 2. Training and Benchmarking a Deep Neural Network Using CIFAR-10"
   ],
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qBYEsnSav5E1"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCy9QqPHv5E3"
   },
   "source": [
    "MemTorch can be used to simulate the inference routines of MDNNs. Consequently, prior to conversion, DNNs must be either defined and trained using PyTorch or imported using PyTorch. \n",
    "\n",
    "In this tutorial, the VGG-16 DNN architecture is trained and benchmarked using the CIFAR-10 image classification data set. \n",
    "* The CIFAR-10 data set consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. \n",
    "* In the cell below, the VG-16 DNN is trained for 50 epochs with a batch size, $\\Im=256$. The initial learning rate is $\\eta = 1e-2$, which is decayed by an order of magnitude every 20 training epochs. \n",
    "* Adam is used to optimize network parameters and Cross Entropy (CE) is used to determine network losses. \n",
    "* `memtorch.utils.LoadCIFAR10` is used to load the CIFAR-10 training and test sets. After each epoch the model is bench-marked using the CIFAR-10 test set. \n",
    "* The model that achieves the highest test set accuracy is saved as *trained_model.pt*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jVH_tu3tv5E4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import memtorch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from memtorch.utils import LoadCIFAR10\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, inflation_ratio=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv0 = nn.Conv2d(in_channels=3, out_channels=128*inflation_ratio, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn0 = nn.BatchNorm2d(num_features=128*inflation_ratio)\n",
    "        self.act0 = nn.ReLU()\n",
    "        self.conv1 = nn.Conv2d(in_channels=128*inflation_ratio, out_channels=128*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=128*inflation_ratio)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(in_channels=128*inflation_ratio, out_channels=256*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=256*inflation_ratio)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.conv3 = nn.Conv2d(in_channels=256*inflation_ratio, out_channels=256*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=256*inflation_ratio)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.conv4 = nn.Conv2d(in_channels=256*inflation_ratio, out_channels=512*inflation_ratio, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=512*inflation_ratio)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.conv5 = nn.Conv2d(in_channels=512*inflation_ratio, out_channels=512, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=512)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.fc6 = nn.Linear(in_features=512*4*4, out_features=1024)\n",
    "        self.bn6 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.act6 = nn.ReLU()\n",
    "        self.fc7 = nn.Linear(in_features=1024, out_features=1024)\n",
    "        self.bn7 = nn.BatchNorm1d(num_features=1024)\n",
    "        self.act7 = nn.ReLU()\n",
    "        self.fc8 = nn.Linear(in_features=1024, out_features=10)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.act0(self.bn0(self.conv0(input)))\n",
    "        x = self.act1(self.bn1(F.max_pool2d(self.conv1(x), 2)))\n",
    "        x = self.act2(self.bn2(self.conv2(x)))\n",
    "        x = self.act3(self.bn3(F.max_pool2d(self.conv3(x), 2)))\n",
    "        x = self.act4(self.bn4(self.conv4(x)))\n",
    "        x = self.act5(self.bn5(F.max_pool2d(self.conv5(x), 2)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.act6(self.bn6(self.fc6(x)))\n",
    "        x = self.act7(self.bn7(self.fc7(x)))\n",
    "        return self.fc8(x)\n",
    "\n",
    "def test(model, test_loader):\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):        \n",
    "        output = model(data.to(device))\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.eq(target.to(device).data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    return 100. * float(correct) / float(len(test_loader.dataset))\n",
    "\n",
    "device = torch.device('cpu' if 'cpu' in memtorch.__version__ else 'cuda')\n",
    "epochs = 50\n",
    "train_loader, validation_loader, test_loader = LoadCIFAR10(batch_size=256, validation=False)\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "best_accuracy = 0\n",
    "for epoch in range(0, epochs):\n",
    "    print('Epoch: [%d]\\t\\t' % (epoch + 1), end='')\n",
    "    if epoch % 20 == 0:\n",
    "        learning_rate = learning_rate * 0.1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.to(device))\n",
    "        loss = criterion(output, target.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy = test(model, test_loader)\n",
    "    print('%2.2f%%' % accuracy)\n",
    "    if accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'trained_model.pt')\n",
    "        best_accuracy = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ag8Z6Rn_v5E8"
   },
   "source": [
    "## 3. Conversion of a Deep Neural Network to a Memristive Deep Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lyt9qHvAv5E9"
   },
   "source": [
    "Within MemTorch, `memtorch.mn.Module.patch_model` can be used to convert a DNN to a MDNN. Prior to conversion, a memristive device model must be defined and characterized in part (prior to the introduction of other non-ideal device characteristics).\n",
    "\n",
    "In the cell below:\n",
    "* A reference memristor from `memtorch.bh.memristor` is defined.\n",
    "* Optional reference memristor keyword arguments are set.\n",
    "* A `memtorch.bh.memristor.Memristor` object is instantiated\n",
    "* The hysteresis loop of the instantiated memristor object is generated/plotted.\n",
    "* The bipolar switching behaviour of the instantiated memristor object is generated/plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "dRMGKP-lv5E-",
    "outputId": "f22a800e-3eae-4060-a874-23d7a218f3cf"
   },
   "outputs": [],
   "source": [
    "reference_memristor = memtorch.bh.memristor.VTEAM\n",
    "reference_memristor_params = {'time_series_resolution': 1e-10}\n",
    "memristor = reference_memristor(**reference_memristor_params)\n",
    "memristor.plot_hysteresis_loop()\n",
    "memristor.plot_bipolar_switching_behaviour()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b0kLErl0v5FC"
   },
   "source": [
    "In the cell below, the trained DNN from Section 2 is converted to an equivalent MDNN, where all linear layers are replaced with memristive-equivalent layers. Specifically:\n",
    "* `memtorch.bh.map.Parameter.naive_map` is used to convert the weights within all `torch.nn.Linear` layers to equivalent conductance values, to be programmed to the two memristive devices used to represent each weight (positive and negative, respectively). \n",
    "* `tile_shape` is set to (128, 128), so that modular crossbar tiles of size 128x128 are used to represent weights. \n",
    "* `ADC_resolution` is set to 8 to set the bit width of all emulated Analogue to Digital Converters (ADC).\n",
    "* `ADC_overflow` is used to set the initial overflow rate of each ADC. \n",
    "* `quant_method` is used to set the quantization method used (linear, by default).\n",
    "* `transistor` is set to `True`, so a 1T1R arrangement is simulated. \n",
    "* `programming_routine` is set to `None` to skip device-level simulation of the programming routine. \n",
    "\n",
    "\n",
    "\n",
    "We note if `transistor` is `False` `programming_routine` must not be `None`. In which case, device-level simulation is performed for each device using `memtorch.bh.crossbar.gen_programming_signal` and `memtorch.bh.memristor.Memristor.simulate`, which use finite differences to model internal device dynamics. As `scheme` is not defined, a double-column parameter representation scheme is adopted. Finally, `max_input_voltage` is 0.3, so inputs to each layer are encoded between -0.3V and +0.3V."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oJWSTW5Qv5FD"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from memtorch.mn.Module import patch_model\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "\n",
    "\n",
    "model = Net().to(device)\n",
    "model.load_state_dict(torch.load('trained_model.pt'), strict=False)\n",
    "patched_model = patch_model(copy.deepcopy(model),\n",
    "                          memristor_model=reference_memristor,\n",
    "                          memristor_model_params=reference_memristor_params,\n",
    "                          module_parameters_to_patch=[torch.nn.Linear],\n",
    "                          mapping_routine=naive_map,\n",
    "                          transistor=True,\n",
    "                          programming_routine=None,\n",
    "                          tile_shape=(128, 128),\n",
    "                          max_input_voltage=1.0,\n",
    "                          ADC_resolution=8,\n",
    "                          ADC_overflow_rate=0.,\n",
    "                          quant_method='linear')"
   ]
  },
  {
   "source": [
    "In the cell below, all patched `torch.nn.Linear` layers are tuned using linear regression. A randomly generated tensor of size (8, `self.in_features`) is propagated through each memristive layer and each legacy layer (accessible using `layer.forward_legacy`). `sklearn.linear_model.LinearRegression` is used to determine the coefficient and intercept between the linear relationship of each set of outputs, which is used to define the `transform_output` lamdba function, that maps the output of each layer to their equivalent representations."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mam3ggffv5FG"
   },
   "outputs": [],
   "source": [
    "patched_model.tune_()"
   ]
  },
  {
   "source": [
    "Finally, in the cell below, the converted and tuned MDNN is benchmarked using the CIFAR-10 data set."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5F0muXPv5FK"
   },
   "outputs": [],
   "source": [
    "print(test(patched_model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eV8IJSH6v5FN"
   },
   "source": [
    "## 4. Modeling non-ideal device characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4H9f4d248V7"
   },
   "source": [
    "After conversion, MemTorch can be used to introduce various non-ideal device characteristics using `memtorch.bh.nonideality.NonIdeality.apply_nonidealities`. Currently, the following non-ideal device characteristics are supported:\n",
    "* `memtorch.bh.nonideality.DeviceFaults`\n",
    "* `memtorch.bh.nonideality.Endurance`\n",
    "* `memtorch.bh.nonideality.FiniteConductanceStates`\n",
    "* `memtorch.bh.nonideality.NonLinear`\n",
    "* `memtorch.bh.nonideality.Retention`\n",
    "\n",
    "Stochastic parameters, used to model process variances, can be defined using `memtorch.bh.StochaticParameter`. The introduction of each type of nonideality is demonstrated below."
   ]
  },
  {
   "source": [
    "# TBD"
   ],
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "x0mghWrdv5FO",
    "outputId": "fb7d0542-7839-438e-94fb-029d05f0bbc1"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2Vcpuuw5D_S"
   },
   "source": [
    "Device-device variability is introduced to the VTEAM reference memristor model using *memtorch.bh.StochasticParameter*, by sampling $R_{\\text{OFF}}$ for each device from a normal distribution with $\\sigma = 20$ and $x = 200$, and  $R_{\\text{ON}}$ for each device from a normal distribution with $\\sigma = 10$ and $x = 100$. Using *np.vectorize*, the $R_{\\text{OFF}}$ and $R_{\\text{ON}}$ values for each memristive device are compared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "colab_type": "code",
    "id": "4thsaVDEv5FS",
    "outputId": "e46021ed-b57f-4be7-f29e-15bd17ec2473"
   },
   "outputs": [],
   "source": [
    "from memtorch.mn.Module import patch_model\n",
    "import copy\n",
    "from memtorch.map.Parameter import naive_map\n",
    "from memtorch.bh.crossbar.Program import naive_program\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "reference_memristor_params = {'time_series_resolution': 1e-10, \n",
    "                              'r_off': memtorch.bh.StochasticParameter(loc=200, scale=20, min=2),\n",
    "                              'r_on': memtorch.bh.StochasticParameter(loc=100, scale=10, min=1)}\n",
    "\n",
    "patched_model_ = patch_model(copy.deepcopy(model),\n",
    "                          memristor_model=reference_memristor,\n",
    "                          memristor_model_params=reference_memristor_params,\n",
    "                          module_parameters_to_patch=[torch.nn.Linear, torch.nn.Conv2d],\n",
    "                          mapping_routine=naive_map,\n",
    "                          transistor=True,\n",
    "                          programming_routine=None,\n",
    "                          max_input_voltage=1.0)\n",
    "\n",
    "A = torch.Tensor(np.vectorize(lambda x: x.r_off)(patched_model_.linear_layer.crossbars[0].devices))\n",
    "B = torch.Tensor(np.vectorize(lambda x: x.r_on)(patched_model_.linear_layer.crossbars[0].devices))\n",
    "C = torch.cat((A, B), 0)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(A.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(B.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "plt.savefig('var.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3okdYrSZ7dJh"
   },
   "source": [
    "We model a number (5) of finite discrete conductance states using $memtorch.bh.nonideality.NonIdeality.FiniteConductanceStates$. The conductance levels within the positive crossbar were compared before and after a finite discrete conductance states are modeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "id": "bpIOctS5v5FV",
    "outputId": "feefbf64-f8fe-4da2-8212-afa17dade373"
   },
   "outputs": [],
   "source": [
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "\n",
    "A = 1 / patched_model.linear_layer.crossbars[0].conductance_matrix\n",
    "model = apply_nonidealities(copy.deepcopy(patched_model),\n",
    "                                  non_idealities=[memtorch.bh.nonideality.NonIdeality.FiniteConductanceStates],\n",
    "                                  conductance_states = 5)\n",
    "B = 1 / model.linear_layer.crossbars[0].conductance_matrix\n",
    "C = torch.cat((A, B), 0)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(A.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(B.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "plt.savefig('finite.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rfyXHayL70HP"
   },
   "source": [
    "We model device failure using $memtorch.bh.nonideality.NonIdeality.DeviceFaults$. The conductance levels within the positive crossbar are once again compared, before and after  50\\% of devices are stuck at $R_{\\text{LRS}}$ and 50\\% of devices are stuck at $R_{\\text{HRS}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "colab_type": "code",
    "id": "E6Xi7FUxv5FZ",
    "outputId": "a074acc6-3e0b-4d39-89cf-90fb2ad05bdd"
   },
   "outputs": [],
   "source": [
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "\n",
    "A = 1 / patched_model.linear_layer.crossbars[0].conductance_matrix\n",
    "model = apply_nonidealities(copy.deepcopy(patched_model),\n",
    "                                  non_idealities=[memtorch.bh.nonideality.NonIdeality.DeviceFaults],\n",
    "                                  lrs_proportion=0.5,\n",
    "                                  hrs_proportion=0.5,\n",
    "                                  electroform_proportion=0)\n",
    "B = 1 / model.linear_layer.crossbars[0].conductance_matrix\n",
    "C = torch.cat((A, B), 0)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(A.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(B.transpose(0, 1), interpolation='nearest', aspect=1, vmin=C.min(), vmax=C.max(), cmap='seismic')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 0')\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "plt.colorbar(cax=cax)\n",
    "plt.savefig('fault.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWX0f_Wz8U1L"
   },
   "source": [
    "We model non-linear I/V characteristics using $memtorch.bh.nonideality.NonIdeality.NonLinear$ during inference. The output of the single *torch.nn.Linear* layer are compared when devices were simulated during inference with linear and non-linear I/V characteristics. Non-linear I/V characteristics were determined by applying a half-voltage sweep, using a sinusoidal cosine voltage signal with a duration of 5ns, amplitude of 1V, and a frequency of 50 MHz to each device. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xIbVHttJv5Fb",
    "outputId": "559d9c25-25f0-4f6c-fbeb-75defceb9048"
   },
   "outputs": [],
   "source": [
    "from memtorch.bh.nonideality.NonIdeality import apply_nonidealities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "A = torch.tensor(np.zeros((100, 4)))\n",
    "B = torch.tensor(np.zeros((100, 4)))\n",
    "for i in range(100):\n",
    "  input = torch.zeros((1,3,8,8)).uniform_(-1, 1)\n",
    "  A[i, :] = patched_model(input)\n",
    "  model = apply_nonidealities(copy.deepcopy(patched_model),\n",
    "                              non_idealities=[memtorch.bh.nonideality.NonIdeality.NonLinear],\n",
    "                              sweep_duration=5e-9,\n",
    "                              sweep_voltage_signal_amplitude=1,\n",
    "                              sweep_voltage_signal_frequency=50e6)\n",
    "  B[i, :] = model(input)\n",
    "\n",
    "print(cosine_similarity([A.view(-1).numpy()], [B.view(-1).numpy()]))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}